\documentclass[11pt]{article}
\usepackage{subfigure}
\usepackage{color}
\usepackage{url}
\usepackage{graphicx}
\usepackage{fullpage}
\usepackage[english]{babel}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{fancyhdr}
\usepackage{hyperref}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{enumerate}
\usepackage{mdframed}
\usepackage{mathrsfs}


\begin{document}
\begin{center}
%-------------------------------------------------------------------------------
%---------------------------------Header----------------------------------------
%-------------------------------------------------------------------------------

\framebox{\parbox{6.5in}{
{\bf{STATS 315B: Data Mining, Spring 2016}}\\
{\bf Homework 3, Due 6/3/2016}\\
{\bf Completed by: Henry Neeb, Christopher Kurrus, Tyler Chase, and Yash Vyas}
}}
\ \\
\end{center}

\section*{Problem 1}

\vspace{5 mm}
\noindent
For a multi-hidden layer problem, note that for our functional form, We would 
have a summation of several compositions (equal to the number of hidden layers 
compositions) of our activation function. We will try to avoid differentiating 
and writing this lengthy composition function because differentiating it and 
expressing it directly will be difficult to do. Instead, we will abstract the 
layers away by define a set of nodes each with directional weighted edges. We 
will find that updating each of the edge's weights will only depend on the 
edge going to and leaving that node. 

\begin{itemize}
\item $w_{ij}^{l} = $ weight from node $i$ in level $(l - 1)$ to node $j$ in 
level $l$.
\item $I_{j}^{l} = $ number of inputs to node $j$ of the $l$th layer = 
the number of units of the previous layer.
\item $o_{i}^{l - 1} = $ the $i$th output from layer $(l -1)$ from the $i$th 
node.
\item $o_{j}^{l} = \frac{1}{1 + exp(-\sum_{i = 0}^{I_{j}^{l}} 
w_{ij}^{l} o_{i}^{l - 1})}$
\item $S = $ our transfer function, which we will take as $S(z) = 
\frac{1}{1 + exp(-z)}$. Note $\frac{\partial S}{\partial z} = S(z)(1 - S(z))$
\item $F(x, w) = $ our model $ = \sum_{i = 0}^{I_{L}} w_{ij}^{L} o_{i}^{L}$
\item $E_{t} = \frac{1}{2}(Y - F(x, w_{t}))^{2} $ Our total error at iteration 
$t$. We square this because we are considering squared error loss. We apply the 
$\frac{1}{2}$ for simplification when we apply a derivative. Scaling by a 
constant should not affect our outcome.
\item $in_{j}^{l} = \sum_{k = 1}^{I_{j}^{l}} w_{kj}^{l} o_{k}^{l - 1} $ = the 
weighted input to node $j$ in layer $l$. By definition, 
$o_{j}^{l} = S(in_{j}^{l})$
\end{itemize}

\vspace{5 mm}
\noindent
When we compute each node's share of the error and thus it's update to the 
weights, we have to first look at the nodes closest to the response/output 
node, compute their share, and then propagate it backwards. This defines a 
recursive relationship: the nodes closest to the input layer needs to 
recursively solve their share of the error dependent on the nodes closer to 
the output layer. We must then define our recursive base case (output layer) 
and our recursive step. We will consider the base case first

\vspace{5 mm}
\noindent
\textbf{Case - Weight Updates to Prediction Node}

\vspace{5 mm}
\noindent
If we are at the prediction node, then our predicted value is the output of 
that node. That is, $F(x, w) = o_{pred}^{L} = S(in_{pred}^{L})$. Then our 
derivation is as follows:

\begin{gather*}
%-------------------------------line 1------------------------------------------
\frac{\partial E_{t}}{\partial w_{kj}^{L}} = 
\frac{\partial E_{t}}{\partial in_{pred}^{L}} 
\frac{\partial in_{pred}^{L}}{\partial w_{kj}^{L}}
\text{, by the chain rule}\\
%-------------------------------line 2------------------------------------------
\frac{\partial E_{t}}{\partial in_{pred}^{L}} = 
\frac{\partial}{\partial in_{pred}^{L}} \frac{1}{2} (Y - F(x, w_{t}))^{2} = 
\frac{\partial}{\partial in_{pred}^{L}} \frac{1}{2} (Y - S(in_{pred}^{L}))^{2} = 
- (Y - S(in_{pred}^{L})) S'(in_{pred}^{L})\\
%-------------------------------line 3------------------------------------------
\text{If we use our definition of } S \text{, } S' = S (1 - S) \text{, so } 
- (Y - S(in_{pred}^{L})) S'(in_{pred}^{L}) = \\
%-------------------------------line 4------------------------------------------
= - (Y - S(in_{pred}^{L})) S(in_{pred}^{L}) (1 - S(in_{pred}^{L}))\\
%-------------------------------line 5------------------------------------------
\frac{\partial in_{pred}^{L}}{\partial w_{kj}^{L}} = 
\frac{\partial}{\partial w_{kj}^{L}} 
\sum_{i = 1}^{I_{L}} w_{ij}^{L} o_{i}^{L - 1}\\
%-------------------------------line 6------------------------------------------
\text{The only term depending on } w_{kj}^{L} \text{ is } o_{k}^{L - 1} 
\text{, so } 
\frac{\partial}{\partial w_{kj}^{L}} 
\sum_{i = 1}^{I_{L}} w_{ij}^{L} o_{i}^{L - 1} = 
o_{k}^{L - 1}\\
%-------------------------------line 7------------------------------------------
\text{In total } 
\frac{\partial E_{t}}{\partial w_{kj}^{L}} = 
\frac{\partial E_{t}}{\partial in_{pred}^{L}} 
\frac{\partial in_{pred}^{L}}{\partial w_{kj}^{L}} = 
- (Y - S(in_{pred}^{L})) S(in_{pred}^{L}) (1 - S(in_{pred}^{L})) o_{k}^{L - 1}\\
%-------------------------------line 8------------------------------------------
\text{Gradient Descent tells us we want to update } w_{kj} \text{ with } 
- \eta \frac{\partial E}{\partial w_{kj}}\\
%-------------------------------line 9------------------------------------------
\text{So, } w_{kj}^{t} = w_{kj}^{t - 1} + 
\eta (Y - S(in_{pred}^{L})) S(in_{pred}^{L}) (1 - S(in_{pred}^{L})) o_{k}^{L - 1} 
\text{ if } w_{kj} \text{ leads to an output node.}\\
%-------------------------------line 10-----------------------------------------
\text{With } S(in_{pred}^{L}) = o_{pred}^{L} \text{, we simplify to:}\\
%-------------------------------line 11-----------------------------------------
w_{kj}^{t} = w_{kj}^{t - 1} + 
\eta (Y - o_{pred}^{L}) o_{pred}^{L} (1 - o_{pred}^{L}) o_{k}^{L - 1} 
\text{ if } w_{kj} \text{ leads to an output node.}
\end{gather*}

\vspace{5 mm}
\noindent
Note that in the above equation, after our forward propagation to reach a 
prediction, we have estimates for each parameter in the update for the 
prediction node, so we can compute this update directly. This will be our 
base case in our recursive algorithm.

\vspace{5 mm}
\noindent
From the above, let us take another another definition: let $U_{pred} = 
\frac{\partial E_{t}}{\partial in_{pred}^{L}}$ and in general 
$U_{k} = \frac{\partial E_{t}}{\partial in_{k}^{l}} = 
- (Y - o_{k}^{l}) o_{k}^{l} (1 - o_{k}^{l})$.

\vspace{5 mm}
\noindent
\textbf{Case - Weight Updates in Hidden Layer}

\vspace{5 mm}
\noindent
Consider a general case for a hidden layer $l$. Note

\begin{gather*}
%-------------------------------line 1------------------------------------------
\frac{\partial E_{t}}{\partial w_{ij}^{l}} = 
\sum_{k = 1}^{I_{l}} 
\frac{\partial E_{t}}{\partial in_{k}^{l}} 
\frac{\partial in_{k}^{l}}{\partial o_{j}^{l - 1}} 
\frac{\partial o_{j}^{l - 1}}{\partial in_{j}^{l - 1}} 
\frac{\partial in_{j}^{l - 1}}{\partial w_{k j}^{l - 1}}
\text{, by the chain rule and fact there are } I_{l} \text{ edges}\\
%-------------------------------line 2------------------------------------------
\text{From prior step, } \frac{\partial E_{t}}{\partial in_{k}^{l}} = 
- (Y - o_{k}^{l}) o_{k}^{l} (1 - o_{k}^{l}) = U_{k}\\
%-------------------------------line 3------------------------------------------
\frac{\partial in_{k}^{l}}{\partial o_{j}^{l - 1}} = 
\frac{\partial}{\partial o_{j}^{l - 1}} 
\sum_{k = 1}^{I_{l - 1}} w_{kj}^{l - 1} o_{k}^{l - 1} = 
w_{j k}^{l - 1} \text{ since the only term depending on } 
o_{j}^{l - 1} \text{ is } w_{j k}^{l - 1}\\
%-------------------------------line 4------------------------------------------
\frac{\partial o_{j}^{l - 1}}{\partial in_{j}^{l - 1}} = 
\frac{\partial}{\partial in_{j}^{l - 1}} S(in_{j}^{l - 1}) = 
S(in_{j}^{l - 1}) (1 - S(in_{j}^{l - 1})) = 
o_{j}^{l - 1} (1 - o_{j}^{l - 1})\\
%-------------------------------line 5------------------------------------------
\frac{\partial in_{j}^{l - 1}}{\partial w_{ij}^{l - 1}} = 
\frac{\partial}{\partial w_{ij}^{l - 1}} 
\sum_{i = 1}^{I_{l - 1}} w_{ij}^{l - 1} o_{i}^{l - 1} = o_{i}^{l - 1} 
\text{ since only dependency on } w_{ij}^{l - 1} 
\text{ is } o_{i}^{l - 1}\\
%-------------------------------line 6------------------------------------------
\text{Then } 
\frac{\partial E_{t}}{\partial w_{i j}^{l - 1}} = 
\sum_{k = 1}^{I_{l}} 
\frac{\partial E_{t}}{\partial in_{k}^{l}} 
\frac{\partial in_{k}^{l}}{\partial o_{j}^{l - 1}} 
\frac{\partial o_{j}^{l - 1}}{\partial in_{j}^{l - 1}} 
\frac{\partial in_{j}^{l - 1}}{\partial w_{i j}^{l - 1}} = \\
%-------------------------------line 7------------------------------------------
= 
\sum_{k = 1}^{I_{l}} 
- (Y - o_{k}^{l}) o_{k}^{l} (1 - o_{k}^{l})
w_{j k}^{l - 1}
o_{j}^{l - 1} (1 - o_{j}^{l - 1}) 
o_{i}^{l - 1} = 
\sum_{k = 1}^{I_{l}} 
U_{k}
w_{j k}^{l - 1}
o_{j}^{l - 1} (1 - o_{j}^{l - 1}) 
o_{i}^{l - 1}\\
%-------------------------------line 8------------------------------------------
\text{Define } U_{j} = 
\sum_{k = 1}^{I_{l}} 
U_{k}
w_{j k}^{l - 1}
o_{j}^{l - 1} (1 - o_{j}^{l - 1}) 
\text{, then } 
\frac{\partial E_{t}}{\partial w_{i j}^{l - 1}} = 
U_{j} o_{i}^{l - 1}\\
%-------------------------------line 8------------------------------------------
\text{And our update is } 
w_{ij}^{t} = w_{ij}^{t - 1} - 
\eta U_{j} o_{i}^{l - 1}
\end{gather*}

\vspace{5 mm}
\noindent
Note that there is a dependency on $U_{j}$ with all other preceding $U_{k}$`s. 
This is our recursive relationship with our base case $U_{k} = U_{pred}$. 
Further, note that the only dependency on the updates are on nodes that connect 
directly to the node we wish to update. Our algorithm is as follows:

\vspace{5 mm}
\noindent
\begin{algorithmic}
\STATE $HelperUpdate(w_{i j}^{t - 1})$
\IF{$j$ in prediction layer}
    \STATE compute and return $U_{j} = - (Y - o_{pred}^{L}) o_{pred}^{L} 
    (1 - o_{pred}^{L})$ 
\ELSE
    \STATE compute and return $\sum_{k = 1}^{I_{l}} 
    HelperUpdate(w_{j k}^{t - 1}) 
    w_{j k}^{l - 1} o_{j}^{l - 1} (1 - o_{j}^{l - 1})$
\ENDIF
\STATE $Update(w_{i j}^{t - 1})$
\STATE return $w_{i j}^{t} = w_{i j}^{t - 1} - 
\eta HelperUpdate(w_{i j}^{t - 1})$
\end{algorithmic}



\end{document}
