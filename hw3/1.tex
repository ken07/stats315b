\documentclass[11pt]{article}
\usepackage{subfigure}
\usepackage{color}
\usepackage{url}
\usepackage{graphicx}
\usepackage{fullpage}
\usepackage[english]{babel}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{fancyhdr}
\usepackage{hyperref}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{enumerate}
\usepackage{mdframed}
\usepackage{mathrsfs}


\begin{document}
\begin{center}
%-------------------------------------------------------------------------------
%---------------------------------Header----------------------------------------
%-------------------------------------------------------------------------------

\framebox{\parbox{6.5in}{
{\bf{STATS 315B: Data Mining, Spring 2016}}\\
{\bf Homework 3, Due 6/3/2016}\\
{\bf Completed by: Henry Neeb, Christopher Kurrus, Tyler Chase, and Yash Vyas}
}}
\ \\
\end{center}

\section*{Problem 1}

\vspace{5 mm}
\noindent
For a multi-hidden layer problem, note that for our funcitonal form, We would 
have a summation of several compositions (equal to the number of hidden layers 
compositions) of our activation function. We will try to avoid differntiating 
and writing this lengthy composition function becuase differentiating it and 
expressing it directly will be difficult to do. Instead, we will abstract the 
layers away by define a set of nodes each with directional weighted edges. We 
will find that updating each of the edge's weights will only depend on the 
edge going to and leaving that node. 

\begin{itemize}
\item $w_{ij}^{l} = $ weight from node $i$ in level $(l - 1)$ to node $j$ in 
level $l$.
\item $I_{j}^{l} = $ number of inputs to node $j$ of the $l$th layer = 
the number of units of the previous layer.
\item $o_{i}^{l - 1} = $ the $i$th output from layer $(l -1)$ from the $i$th 
node.
\item $o_{j}^{l} = \frac{1}{1 + exp(-\sum_{i = 0}^{I_{j}^{l}} 
w_{ij}^{l} o_{i}^{l - 1})}$
\item $S = $ our transfer function, which we will take as $S(z) = 
\frac{1}{1 + exp(-z)}$. Note $\frac{\partial S}{\partial z} = S(z)(1 - S(z))$
\item $z_{j}^{l} = \sum_{i = 0}^{I_{l}} w_{ij}^{l} o_{i}^{l}$ weighted sum of 
input for node $j$ in layer $l$.
\item $F(x, w) = $ our model $ = \sum_{i = 0}^{I_{L}} w_{ij}^{L} o_{i}^{L}$
\item $E_{t} = \frac{1}{2}(Y - F(x, w_{t}))^{2} $ Our total error at iteration 
$t$. We square this because we are considering squared error loss. We apply the 
$\frac{1}{2}$ for simplification when we apply a derivative. Scaling by a 
constant should not affect our outcome.
\item $in_{j}^{l} = \sum_{k = 1}^{I_{l}} w_{kj}^{l} o_{k}^{l - 1} $ = the 
weighted input to node $j$ in layer $l$. By definition, 
$o_{j}^{l} = S(in_{j}^{l})$
\end{itemize}

\vspace{5 mm}
\noindent
When we compute each node's share of the error and thus it's update to the 
weights, we have to first look at the nodes closest to the response/output 
node, compute their share, and then propogate it backwards. This defines a 
recurssive relationship: the nodes closest to the input layer needs to 
recurssively solve their share of the error dependent on the nodes closer to 
the output layer. We must then define our recurrsive base case (output layer) 
and our recurrsive step. We will consider the base case first

\vspace{5 mm}
\noindent
\textbf{Case - Weight Updates to Prediction Node}

\vspace{5 mm}
\noindent
If we are at the prediction node, then our predicted value is the output of 
that node. That is, $F(x, w) = o_{pred}^{L} = S(in_{pred}^{L})$. Then our 
derivation is as follows:

\begin{gather*}
%-------------------------------line 1------------------------------------------
\frac{\partial E_{t}}{\partial w_{kj}^{L}} = 
\frac{\partial E_{t}}{\partial in_{pred}^{L}} 
\frac{\partial in_{pred}^{L}}{\partial w_{kj}^{L}}
\text{, by the chain rule}\\
%-------------------------------line 2------------------------------------------
\frac{\partial E_{t}}{\partial in_{pred}^{L}} = 
\frac{\partial}{\partial in_{pred}^{L}} \frac{1}{2} (Y - F(x, w_{t}))^{2} = 
\frac{\partial}{\partial in_{pred}^{L}} \frac{1}{2} (Y - S(in_{pred}^{L})^{2} = 
- (Y - S(in_{pred}^{L})) S'(in_{pred}^{L})\\
%-------------------------------line 3------------------------------------------
\text{If we use our definition of } S \text{, } S' = S (1 - S) \text{, so } 
- (Y - S(in_{pred}^{L})) S'(in_{pred}^{L}) = \\
%-------------------------------line 4------------------------------------------
= - (Y - S(in_{pred}^{L})) S(in_{pred}^{L}) (1 - S(in_{pred}^{L}))\\
%-------------------------------line 5------------------------------------------
\frac{\partial in_{pred}^{L}}{\partial w_{kj}^{L}} = 
\frac{\partial}{\partial w_{kj}^{L}} 
\sum_{i = 1}^{I_{L}} w_{ij}^{L} o_{i}^{L - 1}\\
%-------------------------------line 6------------------------------------------
\text{The only term depending on } w_{kj}^{L} \text{ is } o_{k}^{L - 1} 
\text{, so } 
\frac{\partial}{\partial w_{kj}^{L}} 
\sum_{i = 1}^{I_{L}} w_{ij}^{L} o_{i}^{L - 1} = 
o_{k}^{L - 1}\\
%-------------------------------line 7------------------------------------------
\text{In total } 
\frac{\partial E_{t}}{\partial w_{kj}^{L}} = 
\frac{\partial E_{t}}{\partial in_{pred}^{L}} 
\frac{\partial in_{pred}^{L}}{\partial w_{kj}^{L}} = 
- (Y - S(in_{pred}^{L})) S(in_{pred}^{L}) (1 - S(in_{pred}^{L})) o_{k}^{L - 1}\\
%-------------------------------line 8------------------------------------------
\text{Gradient Descent tells us we want to update } w_{kj} \text{ with } 
- \eta \frac{\partial E}{\partial w_{kj}}\\
%-------------------------------line 9------------------------------------------
\text{So, } w_{kj}^{t} = w_{kj}^{t - 1} + 
\eta (Y - S(in_{pred}^{L})) S(in_{pred}^{L}) (1 - S(in_{pred}^{L})) o_{k}^{L - 1} 
\text{ if } w_{kj} \text{ leads to an output node.}
\end{gather*}

\vspace{5 mm}
\noindent
Note that in the above equation, after our forward propogation to reach a 
prediction, we have estimates for each parameter in the update for the 
prediction node, so we can compute this update directly. This will be our 
base case in our recurssive algorithm.

\vspace{5 mm}
\noindent
\textbf{Case - Weight Updates Layer Before Prediction Layer}

\begin{gather*}
%-------------------------------line 1------------------------------------------
\frac{\partial E_{t}}{\partial w_{kj}^{L - 1}} = 
\frac{\partial E_{t}}{\partial z_{j}^{L - 1}} 
\frac{\partial z_{j}^{L - 1}}{\partial w_{kj}^{L - 1}}
\text{, by the chain rule}\\
%-------------------------------line 2------------------------------------------
\frac{\partial E_{t}}{\partial z_{j}^{L - 1}} = 
\frac{\partial}{\partial o_{j}^{L - 1}} \frac{1}{2}(Y - F(x, w_{t}))^{2}\\
%-------------------------------line 3------------------------------------------
\text{Note that } F(x, w_{t}) = \sum_{i = 0}^{I_{L}} w_{ij}^{L} o_{i}^{L}\\
%-------------------------------line 4------------------------------------------
\text{Also note } o_{j}^{L} = S(z_{j}^{L - 1}) 
\text{, so } 
\frac{\partial}{\partial z_{j}^{L - 1}} 
\sum_{i = 0}^{I_{L}} w_{ij}^{L} o_{i}^{L} 
\end{gather*}



\begin{gather*}
%-------------------------------line 1------------------------------------------
\text{We can rewrite our } E_{t} \text{ as } 
\frac{1}{2}(Y - F(X; w_{t}))^{2} = 
\frac{1}{2}(Y - in_{pred})^{2} = \\
%-------------------------------line 2------------------------------------------
= \frac{1}{2}(Y - 
\sum_{k: \exists \text{ edge } k \text{ to } pred} w_{k,pred} out_{k,pred})^{2}
\text{ where } pred \text{ is our prediction node}\\
%-------------------------------line 3------------------------------------------
\text{Since we are only concerned with outer edges, need gradients for } 
w_{i, pred}\\
%-------------------------------line 4------------------------------------------
\text{Note } \frac{\partial}{\partial w_{i,pred}}
\sum_{k: \exists \text{ edge } k \text{ to } pred} w_{k,pred} out_{k,pred} = 
out_{i, pred}\\
%-------------------------------line 5------------------------------------------
\text{So } \frac{\partial}{\partial w_{i,pred}} E_{t} = 
(Y - \sum_{k: \exists \text{ edge } k \text{ to } pred} w_{k,pred} out_{k,pred})
out_{i, pred}
\end{gather*}

\end{document}