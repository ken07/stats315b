\documentclass[11pt]{article}
\usepackage{subfigure}
\usepackage{color}
\usepackage{url}
\usepackage{graphicx}
\usepackage{fullpage}
\usepackage[english]{babel}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{fancyhdr}
\usepackage{hyperref}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{enumerate}
\usepackage{mdframed}

\DeclareMathOperator*{\argmin}{argmin}
\newcommand*{\argminl}{\argmin\limits}


\begin{document}
\begin{center}
%-------------------------------------------------------------------------------
%---------------------------------Header----------------------------------------
%-------------------------------------------------------------------------------

\framebox{\parbox{6.5in}{
{\bf{STATS 315B: Data Mining, Spring 2016}}\\
{\bf Homework 1, Due 4/28/2016}\\
{\bf Completed by: Henry Neeb, Christopher Kurrus, Tyler Chase, and Yash Vyas}
}}
\ \\
\end{center}

%-------------------------------------------------------------------------------
%---------------------------------Answer----------------------------------------
%-------------------------------------------------------------------------------

\section*{Problem 4}

\vspace{5 mm} 
\noindent 
Searching over the entire space of function is both computationally infeasible 
(you cannot possibly hope to search over an infinite space) and it would most 
likely yield suboptimal results. We define our target function as follows:

\begin{gather*}
F^{*} = \argminl_{F} R(F) = \argminl_{F} E_{XY}[L(y, F(x))]
\end{gather*}

\noindent
Where:

\begin{itemize}
\item $L(y, F(x)) \ge 0$
\item $y = F(x) \rightarrow L(y, F(x)) = 0$
\end{itemize}

\noindent
In practice, we typically estimate the target function by minimizing the
empirical risk over the training data as a surrogate to minimizing the
prediction risk. So if we develop our model such that $F(x) = y$ for our
training data, we will have found a model such that its total training loss is
$0$ and our empirical risk on the training data is absolutely minimized. There 
is not only just one such model that will fit your training data exactly, but
infinitely many models. Your search for the function that minimizes empirical
risk will then conclude with a set of infinitely many functions to choose from
as your best approximation for the target function.

\vspace{5 mm}
\noindent
Not only will it be infeasible to distinguish between these infinitely many
functions, but all of the functions returned will most likely be nowhere near a
good approximation to the target function. This is due to the fact your training
data is random itself and has noise. When you fit the training data exactly, you
are almost guaranteed to be over-fitting since you will be modeling the noise
exhibited in your particular training data. When you start fitting the noise,
your function loses its ability to predict on new data because that new data may
have the same signal as the old data, but it has different noise. As such, your 
model will exhibit prediction error, and most likely will have more prediction 
error than if you chose a model by restriction the function class to search 
over.
\end{document}