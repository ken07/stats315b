\documentclass[11pt]{article}
\usepackage{subfigure}
\usepackage{color}
\usepackage{url}
\usepackage{graphicx}
\usepackage{fullpage}
\usepackage[english]{babel}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{fancyhdr}
\usepackage{hyperref}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{enumerate}
\usepackage{mdframed}

\DeclareMathOperator*{\argmin}{argmin}
\newcommand*{\argminl}{\argmin\limits}


\begin{document}
\begin{center}
%-------------------------------------------------------------------------------
%---------------------------------Header----------------------------------------
%-------------------------------------------------------------------------------

\framebox{\parbox{6.5in}{
{\bf{STATS 315B: Data Mining, Spring 2016}}\\
{\bf Homework 1, Due 4/28/2016}\\
{\bf Completed by: Henry Neeb, Christopher Kurrus, Tyler Chase, and Yash Vyas}
}}
\ \\
\end{center}

%-------------------------------------------------------------------------------
%---------------------------------Answer----------------------------------------
%-------------------------------------------------------------------------------

\section*{Problem 6}

\vspace{5 mm}
\noindent
Bias and variance are two sources of error that occur when searching for the 
target function. {\bf Bias} is the error associated with your expected model 
not being able to predict the target model. Bias occurs when you make 
assumptions about your target function and restrict your search to a specific 
function class, which almost certainly will not contain the target function. 
Since the target function is outside of the class you are searching over, you 
can never expect to predict the true target function.

\vspace{5 mm}
\noindent
{\bf Variance} is the error associated with how different a model you are 
expected to fit is within a chosen function class. Typically, the more 
restrictive the search space, the less the variance because the possibilities of 
models chosen given your training data is that much less.

\vspace{5 mm}
\noindent
The {\bf bias-variance tradeoff} comes from the tradeoff of trying to be broad 
and flexible in your search of the target function in order to actually be more 
likely to recover it in your search (reducing bias), but not being too 
unrestrictive in your search as to have a volatile model (that is a model that 
will change dramatically with small changes in the training data). That is, 
reducing bias (expanding your search region for the target function), is often 
at odds with reducing variance (restricting your search region to have a more 
stable model). You will often never be able to eliminate both, and when trying 
to reduce your overall error, you will need to make concessions to both types 
of error when evaluating a model.
\end{document}