---
title: "Probelem 2 - Multi-Class Classification: Marketing Data"
author: "Henry Neeb, Christopher Kurrus, Tyler Chase"
date: "April 16, 2016"
output: pdf_document
---

## Libraries

```{r, warning=FALSE, message=FALSE}
library(ggplot2)
library(dplyr)
library(readr)
library(rpart)
library(rpart.plot)
library(RColorBrewer)
library(rattle)
```

## File Parameters

```{r}
path <- "data/"
file <- "Housetype_Data.txt"
ext <- paste(path, file, sep = "")
```


## Read in Data

```{r}
# Name of the variables imported
var_names = c("house_type",
              "sex",
              "marital_status",
              "age",
              "education",
              "occupation",
              "income",
              "bay_duration",
              "dual_income",
              "household_total",
              "household_minors",
              "householder_status",
              "ethnicity",
              "language")

# Type of the variable being imported
var_type <- cols(col_factor(sprintf("%i", 1:5), ordered = FALSE), # house type
                 col_factor(sprintf("%i", 1:2), ordered = FALSE), # sex
                 col_factor(sprintf("%i", 1:5), ordered = FALSE), # marital stat
                 col_factor(sprintf("%i", 1:7), ordered =  TRUE), # age
                 col_factor(sprintf("%i", 1:6), ordered =  TRUE), # education
                 col_factor(sprintf("%i", 1:9), ordered = FALSE), # occupation
                 col_factor(sprintf("%i", 1:9), ordered =  TRUE), # income                 
                 col_factor(sprintf("%i", 1:5), ordered =  TRUE), # bay duration
                 col_factor(sprintf("%i", 1:3), ordered = FALSE), # dual income
                 col_factor(sprintf("%i", 1:9), ordered =  TRUE), # house count
                 col_factor(sprintf("%i", 0:9), ordered =  TRUE), # minor count
                 col_factor(sprintf("%i", 1:3), ordered = FALSE), # rent/own
                 col_factor(sprintf("%i", 1:8), ordered = FALSE), # ethnicity
                 col_factor(sprintf("%i", 1:3), ordered = FALSE)) # language

df <- read_csv(file = ext, 
               col_names = var_names, 
               col_types = var_type)
```

The data does not have any column headers, so we have to manually assign the 
column names. Our variables are also all categorical variables, some with an 
order relationship and some with out. We also have to import these values as 
factors, specifiying if they have an order relationship or not. The following 
variables have an order relationship:

* Income
* Age
* Education
* Amount of time living in the bay area
* Household count
* Minors in household count

The remaining variables do not have an order relationship:

* Sex
* Marital status
* Occupation
* Dual income status
* Whether you rent or own a house
* Type of house you live in
* Ethnicity
* Languages

## Fit a Maximal Tree Model for Pruning

We will first fit a tree with a low complexity paramter so that we can observe 
it's behavior and decide on an optimal parameter based on cross validation 
error. We will use the following control parameters:

* 10-fold cross validation
* 100 minsplit
* Use default surrogate parameters, except split by majority if split data and 
all surrogate information is missing.
* Use a low complexity parameter of 0.00001

```{r}
control_params <- rpart.control(xval = 10L, 
                               minsplit = 100, 
                               usesurrogate = 2,
                               cp = .00001)

fit = rpart(formula = house_type~.,
            method = "class",
            data = df,
            control = control_params)
```

Let us now look at cross-validation error versus our complexity parameter, as 
well as a summary of our model and cross-validation error.

```{r}
printcp(fit)
plotcp(fit)
```

The dashed line on plotcp marks the minimum cross validation error plus one 
standard error. We will use this rule for selecting a model - the model with 
the largest complexity parameter that produces a cross-validation error just 
below the sum of the two aforementioned values.

```{r}
# Find the index where cross validation is minimized to implement 1-SE rule
xerror_list <- fit$cptable[,"xerror"]
min_idx <- as.integer(which.min(xerror_list))
se_cutoff <- as.numeric(fit$cptable[,"xstd"][min_idx]) +
             as.numeric(xerror_list[min_idx])

# Find CP where cross-val error just dips below SE + min cross-val cutoff
opt_cp <- 1.0
for (i in seq_len(length(xerror_list))){
  if (xerror_list[i] < se_cutoff){
    opt_cp <- as.numeric(fit$cptable[,"CP"][i])
    opt_address <- i
    break
  }
}
```

We will now prune the true according to this 1-Standard Error Rule complexity 
parameter and replot and print the complexity parameter.

```{r}
pruned_fit <- prune(tree = fit, cp = opt_cp)

plotcp(pruned_fit)
printcp(pruned_fit)
```

This will be our optimal tree. Lets look at a nice plot.

```{r}
fancyRpartPlot(model = pruned_fit, 
               main = "Pruned Decision Tree", 
               sub = "")
```

In order to get the training error the root node error is multiplied times the relative error. In order to get the test error the root node error is multiplied times the xerror. 

```{r}
root_node_error <- pruned_fit$frame[1, 'dev']/fit$frame[1, 'n']
rel_error <- pruned_fit$cptable[opt_address,"rel error"]
xerror <- pruned_fit$cptable[opt_address,"xerror"]
train_error <- rel_error * root_node_error
test_error <- xerror * root_node_error
```

Therefore the training error of the optimal tree is `r train_error`. The test error of the optimal tree is `r test_error`. 

The optimal tree is plotted above and only has 5 terminal nodes so is relatively easy to interpret. We are attempting to predict a person's house type based on their sex, marital status, age, education, occupation, annual income, how long they have lived in the bay area, dual income (if married), number of persons in household, number of person's in household under 18, household status, ethnic classification, and language spoken at home. If you own your property or live with your parents you most likely live in a house. If you rent and have less than 3 people living in your home, then you most likely rent. If you rent, have 3 or more people living in your home, and make over 20,000 dolars annual income then you most likely live in a house. If you rent, make less than 20,000 dollars annual income, and there 6 or more people living in your residence then you most likely live in a house. If you rent, make less than 20,000 dollars annual income, and have between 3 and 5 people living in your home then you most likely rent. 

From the tree we see that household status, total people in household, and household income are the strongest predictors. 

Lets follow this tree for me. I rent so on the first split travel to the right. I have two roommates so there are three people in my household; so at the second split we travel to the left. I make more than $20,000 a year so at the third split we travel to the left. According to this tree I should be living in a house, however I do actually live in an appartment. So I am missclassified. 