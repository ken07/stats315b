\documentclass[11pt]{article}
\usepackage{subfigure}
\usepackage{color}
\usepackage{url}
\usepackage{graphicx}
\usepackage{fullpage}
\usepackage[english]{babel}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{fancyhdr}
\usepackage{hyperref}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{enumerate}
\usepackage{mdframed}
\usepackage{mathrsfs}


\begin{document}
\begin{center}
%-------------------------------------------------------------------------------
%---------------------------------Problem 1-------------------------------------
%-------------------------------------------------------------------------------

\framebox{\parbox{6.5in}{
{\bf{STATS 315B: Data Mining, Spring 2016}}\\
{\bf Homework 2, Due 5/22/2016}\\
{\bf Completed by: Henry Neeb, Christopher Kurrus, Tyler Chase, and Yash Vyas}
}}
\ \\
\end{center}

\section*{Problem 1}

\vspace{5 mm}
\noindent
The advantage of random variable selection strategy in random forests is that
we reduce the possibility of generating highly correlated bagged trees. Highly
correlated trees lead to low prediction accuracy. By only considering a subset 
of split variables, we force ourselves to split differently at each tree. This 
enhances prediction because if we split on all variables we tend to still split
on the same best variables, so there is little randomness and averaging the 
prediction value won't give us much of a different answer from just making
one tree. The random variable selection strategy works best if the there are 
very few interactions among the features. Another advantage of the method is 
the increased computational efficiency due to less feature exploration. 

\vspace{5 mm}
\noindent
The major disadvantage of random variable selection strategy is not being able 
to capture the interaction effects among the features in most of the bagged 
trees. An interaction between two variables is said to have been captured if 
they are on the samebranch of the generated tree. These interactions are not 
necessarily captured due to random variable selection for the split. Also, one 
does not have the liberty to split on the best feature due to the previous 
constraint. This leads to increasein the bias of the trees generated on average 
and may lead to decrease in prediction accuracy when many trees with low 
predictive power are averaged.

\vspace{5 mm}
\noindent
An alternate startegy to random variable selection in the random forests is to 
randomly change the size of bootstrap sample and allow all variables to be 
split at each node. This strategy introduces variation in the trees due to 
sampling and ensures capturing interaction effects among the features. 


%-------------------------------------------------------------------------------
%---------------------------------Problem 2-------------------------------------
%-------------------------------------------------------------------------------
\newpage
\begin{center}
\framebox{\parbox{6.5in}{
{\bf{STATS 315B: Data Mining, Spring 2016}}\\
{\bf Homework 2, Due 5/22/2016}\\
{\bf Completed by: Henry Neeb, Christopher Kurrus, Tyler Chase, and Yash Vyas}
}}
\ \\
\end{center}

\section*{Problem 2}

\vspace{5 mm}
\noindent
Linear regression uses least squares as the error function. When the number of 
predictors is greater than the number of observations in the training data, we 
have multiple perfect fits for the model and therefore, infinite solutions for 
the coefficient estimates of the least squares. Thus, the model would have a 
huge variance.

\vspace{5 mm}
\noindent
Regularization is necessary to find the unique coefficient estimates when the 
number of observations is less than the number of predictors. If there is high 
collinearity/correlation among the predictors in the model, then a 
regularization such as ridge penalty reduces the variation in coefficient 
estimates of the variables. Another case where regularization helps is when 
there exist multiple noisy variables in the data. In such a case, 
regularization such as a lasso penalty ensures that the coefficients of noisy 
variables are zero.

\vspace{5 mm}
\noindent
Regularization is disadvantageous when the number of predictors is extremely 
small and predictors are believed to be  independent. In this case, we may want 
to pursue feature augmentation by incorporating the higher orders and 
interaction among parameters. But the idea of regularization goes against 
feature augmentation. Another example where regularization fails is when we 
have a perfect line/plane that fits the model for linear regression. Generally, 
regularization fails when the bias introduced by it does not lead to drastic 
decrease in the variance of prediction. 

\vspace{5 mm}
\noindent
During boosting, we generate a linear combination of all possible trees. In 
this situation sparsity is assumed because out of all possible trees there are 
only a few trees that capture the signal in the data. 

\vspace{5 mm}
\noindent
Most of the trees generated in boosting do not influence the outcome variable. 
If we assume sparsity and it exists then our predictions are good; otherwise, 
our predictions are inaccurate. However, if we do not assume sparsity and 
our variable space is larger than our sample space we are stuck. We simply do 
not have enough data to estimate a large number of parameters (all possible 
trees in the boosting context). Hence, the assumption of sparsity is always 
reasonable in boosting.

%-------------------------------------------------------------------------------
%---------------------------------Problem 3-------------------------------------
%-------------------------------------------------------------------------------
\newpage
\begin{center}
\framebox{\parbox{6.5in}{
{\bf{STATS 315B: Data Mining, Spring 2016}}\\
{\bf Homework 2, Due 5/22/2016}\\
{\bf Completed by: Henry Neeb, Christopher Kurrus, Tyler Chase, and Yash Vyas}
}}
\ \\
\end{center}

\section*{Problem 3}

\noindent
For convex empirical risk and convex penalities, the solution of 
$\hat{a}(\lambda)$ will be unique since it is a sum of convex functions. We 
will now attempt to minimize the sum of the empircal risk and the penalty. 
First, we consider minimizing the empirical risk:

%-------------------------------------------------------------------------------
%------------------------Emperical Risk Derivative------------------------------
%-------------------------------------------------------------------------------
\begin{gather*}
%------------------------------line 1-------------------------------------------
\text{Let } L(y_{i}, a_{0} + \sum_{j = 1}^{n} a_{j}x_{ij}) = L 
\text{ and } a_{0} + \sum_{j = 1}^{n} a_{j}x_{ij} = f(x, a) = f\\
%------------------------------line 2-------------------------------------------
\frac{\partial \hat{R}}{\partial a_{k}} = 
\frac{1}{N} \sum_{j = 1}^{N} \frac{\partial L}{\partial a_{k}}\\
%------------------------------line 3-------------------------------------------
\text{By the chain rule: } 
\frac{\partial L}{\partial a_{k}} = 
\frac{\partial L(y_{i}, f(x, a))}{\partial a_{k}} = 
\frac{\partial L(y_{i}, f(x, a))}{f(x, a)} 
\frac{\partial f(x, a)}{\partial a_{k}} = 
\frac{\partial L}{\partial f} \frac{\partial f}{\partial a_{k}}\\
%------------------------------line 4-------------------------------------------
\text{Now consider } 
\frac{\partial f}{\partial a_{k}} = 
\frac{\partial}{\partial a_{k}} [a_{0} + \sum_{j = 1}^{n} a_{j}x_{ij}]\\
%------------------------------line 5-------------------------------------------
\text{Note that } a_{k} 
\text{ only appears in the sumation and is multiplied by } x_{ik}\\
%------------------------------line 6-------------------------------------------
\text{So all values in } f 
\text{ will evaluate to zero except } a_{k}x_{ik}\\
%------------------------------line 7-------------------------------------------
\text{Then } 
\frac{\partial}{\partial a_{k}} [a_{0} + \sum_{j = 1}^{n} a_{j}x_{ij}] = x_{ik}\\
%------------------------------line 8-------------------------------------------
\text{And } 
\frac{1}{N} \sum_{i = 1}^{N} \frac{\partial L}{\partial a_{k}} = 
\frac{1}{N} \sum_{i = 1}^{N} \frac{\partial L}{\partial f} 
\frac{\partial f}{\partial a_{k}} = 
\frac{1}{N} \sum_{i = 1}^{N} \frac{\partial L}{\partial f} x_{ik}
\end{gather*}

\vspace{5mm}
\noindent
Now we consider the penalty function for $\gamma \ge 1$:

%-------------------------------------------------------------------------------
%---------------------------Penalty Derivative----------------------------------
%-------------------------------------------------------------------------------
\begin{gather*}
%------------------------------line 1-------------------------------------------
\frac{\partial P_{\gamma} (a)}{\partial a_{k}} = 
\frac{\partial}{\partial a_{k}} \sum_{j = 1}^{n} |a_{j}|^{\gamma}\\
%------------------------------line 2-------------------------------------------
\text{Note, the only value in this sum that depends on } a_{k} 
\text{ is } |a_{k}|^{\gamma}\\
%------------------------------line 3-------------------------------------------
\text{So, } \frac{\partial}{\partial a_{k}} \sum_{j = 1}^{n} |a_{j}|^{\gamma} = 
\frac{\partial}{\partial a_{k}} |a_{k}|^{\gamma} = 
\gamma |a_{k}|^{\gamma - 1} \frac{\partial |a_{k}|}{\partial a_{k}} 
\text{, where } \frac{\partial |a_{k}|}{\partial a_{k}} 
\text{ is the subgradient and } 
\frac{\partial |a_{k}|}{\partial a_{k}} \in [-1, 1]\\
\end{gather*}

\newpage

\noindent
Note that the above holds for $\gamma = 1$. Finally, together we have:

%-------------------------------------------------------------------------------
%---------------------------Combined Derivative---------------------------------
%-------------------------------------------------------------------------------
\begin{gather*}
\frac{\partial}{\partial a_{k}} [\hat{R}(a) + \lambda P_{\gamma}(a)] = 
\frac{1}{N} \sum_{i = 1}^{N} \frac{\partial L}{\partial f} x_{ik} + 
\gamma \lambda |a_{k}|^{\gamma - 1} \frac{\partial |a_{k}|}{a_{k}}
\end{gather*}

\vspace{5mm}
\noindent
To find the minimum, we would need to set the above equal to zero and find the 
values of $a$ that satisfy the equality. Now, let's assume that $\gamma > 1$. 
We will show by contradiction that values of $a$ are not forced to zero. Assume 
that some $a_{k} = 0$, then:

%-------------------------------------------------------------------------------
%---------------------------Proof by Contradiction------------------------------
%-------------------------------------------------------------------------------
\begin{gather*}
%------------------------------line 1-------------------------------------------
\frac{1}{N} \sum_{i = 1}^{N} \frac{\partial L}{\partial f} x_{ik} + 
\gamma |a_{k}|^{\gamma - 1} \frac{\partial |a_{k}|}{a_{k}} = 
\frac{1}{N} \sum_{i = 1}^{N} \frac{\partial L}{\partial f} x_{ik} + 
\gamma \lambda \times 0^{\gamma - 1} \times \frac{\partial |a_{k}|}{a_{k}} = 
%------------------------------line 2-------------------------------------------
\frac{1}{N} \sum_{i = 1}^{N} \frac{\partial L}{\partial f} x_{ik}\\
\text{Then } 
\frac{\partial}{\partial a_{k}} [\hat{R}(a) + \lambda P_{\gamma}(a)] = 0 \implies 
\frac{1}{N} \sum_{i = 1}^{N} \frac{\partial L}{\partial f} x_{ik} = 0
\end{gather*}

\vspace{5mm}
\noindent
Note that the above is just the derivative of the emperical risk set equal to 
zero. Its solution then will just be the standard unpenalized solution and 
will not necessarily equal zero. This is a contradiction to our assumption that 
$a_{k} = 0$, thus $a_{k} \ne 0$ and all coefficients along the path indexed by 
$\lambda$ are not necessarily equal to zero.

\vspace{5mm}
\noindent
We will now show that getting zero coefficients is possible for the lasso 
penalty. Assume $\gamma = 1$.

%-------------------------------------------------------------------------------
%-------------------------------Lasso Proof-------------------------------------
%-------------------------------------------------------------------------------
\begin{gather*}
%------------------------------line 1-------------------------------------------
\frac{1}{N} \sum_{i = 1}^{N} \frac{\partial L}{\partial f} x_{ik} + 
\gamma \lambda |a_{k}|^{\gamma - 1} \frac{\partial |a_{k}|}{a_{k}} = 
\frac{1}{N} \sum_{i = 1}^{N} \frac{\partial L}{\partial f} x_{ik} + 
\lambda \frac{\partial |a_{k}|}{a_{k}}\\
%------------------------------line 2-------------------------------------------
\text{For } a_{k} \text{ near zero } 
-1 \le \frac{\partial |a_{k}|}{a_{k}} \le 1 \implies 
-\lambda \le -\lambda \frac{\partial |a_{k}|}{a_{k}} \le \lambda\\
%------------------------------line 3-------------------------------------------
\text{So } 
\frac{\partial}{\partial a_{k}} [\hat{R}(a) + \lambda P_{\gamma}(a)] = 0 \implies 
-\lambda \frac{\partial |a_{k}|}{a_{k}} = 
\frac{1}{N} \sum_{i = 1}^{N} \frac{\partial L}{\partial f} x_{ik} \implies 
-\lambda \le \frac{1}{N} \sum_{i = 1}^{N} \frac{\partial L}{\partial f} x_{ik} \le \lambda
\end{gather*}

\vspace{5mm}
\noindent
So, for $a_{k}$ near zero, we can bound the solution to the derivative of the 
unpenalized loss between plus and minus $\lambda$. This means that the solution 
will be equal to zero between those values of $\lambda$, so we have shown that 
$a_{k}$ can be zero with the lasso penalty.

\vspace{5mm}
\noindent
We will now show that the elastic net has the ability to set some coefficients 
to zero.

%-------------------------------------------------------------------------------
%-------------------Elastic Net Penalty Derivative------------------------------
%-------------------------------------------------------------------------------
\begin{gather*}
%------------------------------line 1-------------------------------------------
\frac{\partial P_{\gamma}(a)}{\partial a_{k}} = 
\frac{\partial}{\partial a_{k}}[
\sum_{j = 1}^{n} (\gamma - 1) \frac{a_{j}^{2}}{2} + 
(2 - \gamma) |a_{j}|
]\\
%------------------------------line 2-------------------------------------------
\text{Again, the only values that will not evaluate to zero are the } 
a_{k} 
\text{ values. So the above evaluates to:}\\
%------------------------------line 3-------------------------------------------
(\gamma - 1) a_{k} + (2 - \gamma) \frac{\partial |a_{k}|}{a_{k}}
\end{gather*}

\vspace{5mm}
\noindent
We have shown what will happen for $\gamma = 1$ (Lasso penalty) and for 
$\gamma = 2$ (Ridge penalty). We only need to consider $1 < \gamma < 2$. With 
these constraints, we are interested in solving:

%-------------------------------------------------------------------------------
%-------------------Proof Elastic Net Forces Zero-------------------------------
%-------------------------------------------------------------------------------
\begin{gather*}
%------------------------------line 1-------------------------------------------
\frac{\partial}{\partial a_{k}} [\hat{R}(a) + \lambda P_{\gamma}(a)] = 
\frac{1}{N} \sum_{i = 1}^{N} \frac{\partial L}{\partial f} x_{ik} + 
\lambda (\gamma - 1) a_{k} + 
\lambda (2 - \gamma) \frac{\partial |a_{k}|}{a_{k}}\\
%------------------------------line 2-------------------------------------------
\text{For } a_{k} \text{ near zero } 
-1 \le \frac{\partial |a_{k}|}{a_{k}} \le 1 \implies 
-\lambda (2 - \gamma) \le 
-\lambda (2 - \gamma) \frac{\partial |a_{k}|}{a_{k}} \le 
\lambda (2 - \gamma)\\
%------------------------------line 3-------------------------------------------
\text{Then } 
\frac{1}{N} \sum_{i = 1}^{N} \frac{\partial L}{\partial f} x_{ik} + 
\lambda (\gamma - 1) a_{k} + 
\lambda (2 - \gamma) \frac{\partial |a_{k}|}{a_{k}} = 0 \implies\\
%------------------------------line 4-------------------------------------------
\implies
\frac{1}{N} \sum_{i = 1}^{N} \frac{\partial L}{\partial f} x_{ik} + 
\lambda (\gamma - 1) a_{k} = 
- \lambda (2 - \gamma) \frac{\partial |a_{k}|}{a_{k}} \implies\\
%------------------------------line 5-------------------------------------------
\implies
-\lambda (2 - \gamma) \le 
\frac{1}{N} \sum_{i = 1}^{N} \frac{\partial L}{\partial f} x_{ik} + 
\lambda (\gamma - 1) a_{k} \le
\lambda (2 - \gamma)
\end{gather*}

\vspace{5mm}
\noindent
The bounded value is a ridge-like solution (with 
$\lambda ' = (\gamma - 1) \lambda$). $a_{k}$ solutions to this near zero can 
be bounded by $\lambda (2 - \gamma)$, which implies that some coefficients can 
be forced to zero (by the same logic as why the lasso forces some coefficients 
to zero).

%-------------------------------------------------------------------------------
%---------------------------------Problem 4-------------------------------------
%-------------------------------------------------------------------------------
\newpage
\begin{center}

\framebox{\parbox{6.5in}{
{\bf{STATS 315B: Data Mining, Spring 2016}}\\
{\bf Homework 2, Due 5/22/2016}\\
{\bf Completed by: Henry Neeb, Christopher Kurrus, Tyler Chase, and Yash Vyas}
}}
\ \\
\end{center}

\section*{Problem 4}

Assume $E[x_j] = 0$ and $E[x_j^2] = 1$ for all x.

We want to show that the variable, $x_j^*$ that has the maximum absolute 
correlation with y 

\begin{equation}
j^* = \underset{1 \leq j \leq J}{\operatorname{argmax}}\hspace{1mm}|E(y \cdot x)|
\end{equation}

is the same as the one that best predicts y using squared error loss

\begin{equation}
j^* = \underset{1 \leq j \leq J}{\operatorname{argmin}} 
\hspace{1mm} \underset{\rho}{\operatorname{min}} \hspace{1mm} E[y - \rho x_j]^2
\end{equation}

We begin by expanding equation 2. 

$$j^* = \underset{1 \leq j \leq J}{\operatorname{argmin}} 
\hspace{1mm} \underset{\rho}{\operatorname{min}} 
\hspace{1mm} E[(y - \rho x_j)^T ( y- \rho x_j)]$$ 

$$j^* = \underset{1 \leq j \leq J}{\operatorname{argmin}} 
\hspace{1mm} \underset{\rho}{\operatorname{min}} 
\hspace{1mm} E[y^Ty + \rho^2 x_j^T x_j - \rho x_j^T y - \rho y^T x_j]$$ 

Distributing the expectation as well as using our second assumption leaves us 
with

$$j^* = \underset{1 \leq j \leq J}{\operatorname{argmin}} 
\hspace{1mm} \underset{\rho}{\operatorname{min}} 
\hspace{1mm} [E(y^2) + \rho^2 - 2 \rho E(y \cdot x_j)]$$

Since $y$ and $\rho$ are real $\Rightarrow$ $E(y^2) \geq 0$ and $\rho^2 \geq 0$ 

define $A = E(y^2) + \rho^2 \geq 0$

$$j^* = \underset{1 \leq j \leq J}{\operatorname{argmin}} 
\hspace{1mm} \underset{\rho}{\operatorname{min}} 
\hspace{1mm} [A - 2 \rho E(y \cdot x_j)]$$

if $E(y \cdot x_j) < 0$ want to minimize $\rho |E(y \cdot x_j)|$ 

if $E(y \cdot x_j) > 0$ want to maximize $\rho |E(y \cdot x_j)|$ 

First we want to minimize with respect to $\rho$ to get $\rho^*$

$$\frac{\partial}{\partial \rho} \bigg[ E(y^2) + 
\rho^2 - 2 \rho E(y \cdot x_j) \bigg] = 
2 \rho - 2 E(y \cdot x_j) \Rightarrow \rho^* = 
E(y \cdot x_j) \Rightarrow sign(\rho^*) = sign(E(y \cdot x_j))$$ 

if $E(y \cdot x_j) < 0$ want to minimize $- |\rho^*| |E(y \cdot x_j)|$

if $E(y \cdot x_j) > 0$ want to maximize $ |\rho^*| |E(y \cdot x_j)|$

Therefore in either case we want to maximize $|E(y \cdot x_j)|$

$$ \Rightarrow j^* = 
\underset{1 \leq j \leq J}{\operatorname{argmax}}\hspace{1mm} |E(y \cdot x_j)|$$

%-------------------------------------------------------------------------------
%---------------------------------Problem 5-------------------------------------
%-------------------------------------------------------------------------------
\newpage
\begin{center}
\framebox{\parbox{6.5in}{
{\bf{STATS 315B: Data Mining, Spring 2016}}\\
{\bf Homework 2, Due 5/22/2016}\\
{\bf Completed by: Henry Neeb, Christopher Kurrus, Yash Vyas and Tyler Chase}
}}
\ \\
\end{center}

\section*{Problem 5}

\vspace{1 mm}
\noindent
Taking the expectation of our additive function, we have:

\begin{gather*}
E_{z_{\backslash l}}[F(z_{l}, z_{\backslash l})] = 
E_{z_{\backslash l}}[F(z_{l}) + F(z_{\backslash l})] = 
E_{z_{\backslash l}}[F(z_{l})] + E_{z_{\backslash l}}[F(z_{\backslash l})]\\
\text{Note that } E_{z_{\backslash l}}[F(z_{\backslash l})] 
\text{evaluates to a constant}\\
\text{Also, } E_{z_{\backslash l}}[F(z_{l})] = F(z_{l})\\
\text{because we are taking an expectation with respect to a variable not present in the function}\\
\text{So we have } 
E_{z_{\backslash l}}[F(z_{l})] + E_{z_{\backslash l}}[F(z_{\backslash l})] = 
F(z_{l}) + constant
\end{gather*}

\vspace{5 mm}
\noindent
So we have shown that this expectation evaluates to a function of $F(z_{l})$ 
up to an additive constant. Now we will show what $E[F(x)|z_{l}]$ evaluates to:


\begin{gather*}
E_{x|z_{l}}[F(x)|z_{l}] = E_{x|z_{l}}[[F(z_{l}) + F(z_{\backslash l})]|z_{l}] = 
E_{x|z_{l}}[F(z_{l})|z_{l}] + E_{x|z_{l}}[F(z_{\backslash l})|z_{l}]\\
\text{Note } E_{x|z_{l}}[F(z_{l})|z_{l}] = F(z_{l}) \text{ and } 
E_{x|z_{l}}[F(z_{\backslash l})|z_{l}] \text{ is a function of } z_{\backslash l}\\
\text{If } z_{l} \text{ and } z_{\backslash l} \text{ are independent, then }
E_{x|z_{l}}[F(z_{\backslash l})|z_{l}] = E_{x|z_{l}}[F(z_{\backslash l})]\\
\text{Then in general, } E_{x|z_{l}}[F(x)|z_{l}] = 
F(z_{l}) + E_{x|z_{l}}[F(z_{\backslash l})|z_{l}] = F(z_{l}) + E_{z_{\backslash l}}[F(z_{\backslash l})]\\
\text{However, if } z_{l} \text{ and } z_{\backslash l} \text{ are independent:}\\
E_{x|z_{l}}[F(x)|z_{l}] = F(z_{l}) + constant
\end{gather*}

\vspace{5 mm}
\noindent
Thus the dependence of $F(x)$ on $z_{l}$ ignoring the other variables 
($z_{\backslash l}$) will be the same as the partial dependence of $F(x)$ on 
$z_{l}$ when the two are independent.

\end{document}