\documentclass[11pt]{article}
\usepackage{subfigure}
\usepackage{color}
\usepackage{url}
\usepackage{graphicx}
\usepackage{fullpage}
\usepackage[english]{babel}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{fancyhdr}
\usepackage{hyperref}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{enumerate}
\usepackage{mdframed}
\usepackage{mathrsfs}


\begin{document}
\begin{center}
%---------------------------------------------------------------------------------------
%---------------------------------Header------------------------------------------------
%---------------------------------------------------------------------------------------

\framebox{\parbox{6.5in}{
{\bf{STATS 315B: Data Mining, Spring 2016}}\\
{\bf Homework 2, Due 5/22/2016}\\
{\bf Completed by: Henry Neeb, Christopher Kurrus, Tyler Chase, and Yash Vyas}
}}
\ \\
\end{center}

\section*{Problem 2}

\vspace{5 mm}
\noindent
Linear regression uses least squares as the error function. When the number of predictors is
greater than the number of observations in the training data, there exist multiple perfect fits for the
model leading to infinite solutions for the coefficient estimates of the least squares. Thus, the model
would have a huge varaince.

Regularization is necessary to find the unique coefficient estimates. It also helps in cases when there is a high 
collinearity/correlation among the predictors in the model. Another case where regularisation helps is when we
have many noisy variables in the data. In such a case, regularization decreases the coefficients estimates and of
the noisy variables. 

Regularization is disadvantageous when the number predictors is extremely small. In this case, one would like 
to pursue feature augmentation which is not supported by regularization. Another example where regularization 
fails is when we have a perfect line that fits the model. Generally, regularization fails when the bias introduced 
by it does not decrease the variance in prediction. 

Sparsity is a reasonable assumption in boosting because we generate multiple trees, not all of which 
capture the signal in the data. It is always a reasonable assumption in the context because most of the 
trees generated during boosting do not influence the outcome variable. Most of the times, we simply do
not have enough the data to estimate a large number of parameters (all possible trees in the boosting
context).


\vspace{5 mm}
\noindent


\end{document}