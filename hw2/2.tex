\documentclass[11pt]{article}
\usepackage{subfigure}
\usepackage{color}
\usepackage{url}
\usepackage{graphicx}
\usepackage{fullpage}
\usepackage[english]{babel}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{fancyhdr}
\usepackage{hyperref}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{enumerate}
\usepackage{mdframed}
\usepackage{mathrsfs}


\begin{document}
\begin{center}
%---------------------------------------------------------------------------------------
%---------------------------------Header------------------------------------------------
%---------------------------------------------------------------------------------------

\framebox{\parbox{6.5in}{
{\bf{STATS 315B: Data Mining, Spring 2016}}\\
{\bf Homework 2, Due 5/22/2016}\\
{\bf Completed by: Henry Neeb, Christopher Kurrus, Tyler Chase, and Yash Vyas}
}}
\ \\
\end{center}

\section*{Problem 2}

\vspace{5 mm}
\noindent
Linear regression uses least squares as the error function. When the number of predictors is
greater than the number of observations in the training data, there exists a perfect fit for the
model in the high dimension. We do not have a unique coefficient estimate of the least squares.

This model would most certainly leads to high errors on the test data set. Regularization is necessary
to find the unique coefficient estimates. It also helps in cases when there is a high collinearity/correlation
among the predictors in the model.

Regularization is disadvantageous when the number predictors are extremely small. In this case, the bias
introduced by regularzation might not offset the decrease in variance. 
Regularization constraints the estimated coefficients and decreases the variance of prediction 
at the cost of slight increase in the bias. is necessary to make the 
decreases the variablilty of the model by penalizing on the size of the 
predictors. 


\vspace{5 mm}
\noindent


\end{document}